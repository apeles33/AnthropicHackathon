# 150 ML Research Project Ideas

## Paper Recreation (Type 1)
1. Self-Supervised Vision Transformers (DINO) - scaled down
2. MoCo v3: Momentum Contrast self-supervised learning
3. SimCLR contrastive learning on CIFAR-10
4. CLIP-style text-image contrastive (mini version)
5. MAE (Masked Autoencoder) image reconstruction
6. DeiT: Distillation with vision transformers
7. CoAtNet: Convolution + attention combination
8. EfficientNetV2 architecture improvements
9. Swin Transformer windowed attention
10. ConvNeXt: Modern ConvNet design

## Architecture from Scratch (Type 2)
11. AlexNet on CIFAR-10 with modern improvements
12. VGG-16 depth vs width trade-offs
13. ResNet-18/34/50 residual connections
14. Inception multi-scale features
15. DenseNet connectivity patterns
16. MobileNet depthwise separable convolutions
17. SqueezeNet Fire modules
18. EfficientNet compound scaling
19. Transformer encoder for classification
20. U-Net for segmentation
21. BERT-tiny sentiment (2-layer)
22. GPT-mini text generation (4-layer)
23. Seq2Seq with attention for translation
24. Character-level RNN
25. Conv-TasNet audio separation

## Fine-Tuning (Type 3)
26. BERT for NER
27. BERT for QA
28. BERT text classification
29. RoBERTa sentiment
30. DistilBERT efficient NLP
31. GPT-2 domain-specific generation
32. T5 summarization
33. ResNet medical images
34. EfficientNet plant disease
35. CLIP zero-shot classification
36. ViT fine-grained recognition
37. Whisper speech recognition
38. ELECTRA token classification
39. DeBERTa disentangled attention
40. Layer-wise learning rates

## Training Innovation (Type 4)
41. Curriculum learning: easy to hard
42. Mixup data augmentation
43. CutMix patch mixing
44. AutoAugment learned strategies
45. RandAugment random augmentation
46. Label smoothing effects
47. Cosine annealing LR
48. Warm restarts (SGDR)
49. SAM optimizer: sharpness-aware
50. AdamW vs Adam comparison
51. Gradient accumulation
52. Mixed precision (FP16)
53. Progressive resizing
54. Knowledge distillation
55. Self-training pseudo-labels

## Interpretability (Type 5)
56. Attention weight visualization
57. Grad-CAM saliency
58. Integrated gradients
59. LIME explanations
60. SHAP values
61. Activation maximization
62. Filter visualization
63. t-SNE embeddings
64. UMAP high-dimensional viz
65. Layer-wise relevance
66. Attention flow across layers
67. Counterfactual explanations
68. Concept activation vectors
69. Influence functions
70. Adversarial as interpretability

## Synthetic Data (Type 6)
71. DCGAN on MNIST
72. WGAN with gradient penalty
73. StyleGAN-inspired simplified
74. Conditional GAN
75. CycleGAN image-to-image
76. VAE generation
77. β-VAE disentangled
78. Diffusion models DDPM
79. Score-based generative
80. Normalizing flows
81. SMOTE for tabular
82. Synthetic time series ARIMA
83. Neural style transfer augmentation
84. Text generation char-RNN
85. Tabular GAN CTGAN

## RL Environment (Type 7)
86. CartPole DQN
87. MountainCar policy gradient
88. LunarLander Actor-Critic
89. Custom grid world navigation
90. Multi-armed bandit
91. Q-learning FrozenLake
92. SARSA vs Q-learning
93. MCTS game playing
94. Pendulum DDPG
95. Resource allocation env
96. Atari Pong DQN
97. Trading environment PPO
98. Inventory management RL
99. Multi-agent coordination
100. Curriculum RL progressive

## Loss Function Design (Type 8)
101. Focal loss imbalanced
102. Contrastive metric learning
103. Triplet loss faces
104. Center loss compactness
105. ArcFace angular margin
106. Dice loss segmentation
107. Lovász-Softmax IoU
108. Smooth L1 vs MSE
109. Huber loss robust
110. Quantile loss regression
111. Custom multi-objective
112. Adversarial robustness loss
113. Perceptual loss features
114. Feature matching GANs
115. Ranking loss IR

## Model Compression (Type 9)
116. Quantization INT8
117. Pruning structured
118. Knowledge distillation
119. Size-accuracy tradeoff
120. Inference speedup

## Few-Shot Learning (Type 10)
121. Prototypical networks
122. MAML meta-learning
123. Matching networks
124. Siamese networks
125. N-way K-shot evaluation

## Transfer Learning (Type 11)
126. Medical image adaptation
127. Language domain transfer
128. Feature extraction vs fine-tuning
129. Layer freezing experiments
130. Cross-domain performance

## Neural Architecture Search (Type 12)
131. Random search baseline
132. Evolutionary algorithms
133. RL-based search
134. Grid search comparison
135. Architecture diversity

## Adversarial ML (Type 13)
136. FGSM attack
137. PGD iterative attack
138. C&W optimization attack
139. Adversarial training defense
140. Robust accuracy evaluation

## Multi-Modal (Type 14)
141. Text+Image early fusion
142. Audio+Text late fusion
143. Cross-modal attention
144. Modality importance
145. Unified embeddings

## Continual Learning (Type 15)
146. EWC elastic weight
147. LwF learning without forgetting
148. Replay buffer
149. Progressive networks
150. Task-wise metrics

Each idea provides enough specificity to guide implementation while leaving room for creative parameter choices.
